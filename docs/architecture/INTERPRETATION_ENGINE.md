# Interpretation Engine

## Overview

The Interpretation Engine (`services/interpretation/`) generates human-readable narrative insights from structured aggregation and attribution data.

## Core Philosophy: "Zero-Hype"

We prefer **silence** over **noise**.
- If data is ambiguous → Say nothing / "Monitoring".
- If data is volatile → Warn about uncertainty.
- If data is clear → Provide specific, grounded action.

## Pipeline Architecture

```
1. Fetch Context (Aggregates, Attribution, Historic Interpretations)
2. Select Provider (OpenAI, Anthropic, or Disabled)
3. Generate JSON (Structured Output)
4. Validate Schema (Zod parse)
5. Verify Grounding (Hallucination check)
6. Persist
```

## Safety Mechanisms

### 1. Grounding Contract
Every claim generated by the LLM must be accompanied by a `grounding_map`.
- **Claim**: "Workload is the primary stressor."
- **Reference**: `{"claim_id": "c1", "evidence": ["drivers.internal.workload_volume.score"]}`

If the evidence path does not exist or the value doesn't support the claim (e.g., score is actually good), the pipeline **REJECTS** the entire interpretation.

### 2. Provider Abstraction
We define a strict `LLMProvider` interface.
- Switch models without changing business logic.
- `MockProvider` for deterministic testing.
- `CircuitBreaker` for API failures.

### 3. Concurrency Gating
LLM calls are expensive and rate-limited.
- The `WeeklyRunner` limits concurrency (default: 2 parallel calls per org).
- Queuing logic ensures we don't hit 429 errors.

## Sections Generated

1. **Executive Summary**: High-level status (1-2 sentences).
2. **Key Drivers**: Top 3 influencing factors (Internal or External).
3. **Risks**: Propagation or burnout risks.
4. **Action Recommendations**: Specific method-based interventions.
5. **Questions**: Suggested 1-on-1 questions for team leads.

## Idempotency

The interpretation request is hashed:
`hash(week + org + team + aggregate_snapshot_hash)`

If the hash matches an existing record in `weekly_interpretations`, the service returns the cached result immediately. This saves money and ensures stable UI.
